Keywords in HLD

LOAD BALANCER
1. Server failover:   		                    fast switch to other server, solved by load balancer
2. Scale up :    			                        verticals scaling
3. Scale out:    			                        Horizontal scaling
4. Sticky session(stateful Arch):             If UserA’s session data is stored in server A, Auth request should redirect to server A only.  Adding removing server difficult.

DATABASE REPLICATION
1. Reliability: 		                        achieved through database replication through master slave
2. High availability                        if db gets down redirected to other master or slave server.
3. CDN(Content delivery Network)            store static contents (image, css, javascript, image) in CDNs.
4. Sharding        		                      scale db, hash function used to find the associated shard, ex. user_id%4. Choose sharing key (one or more column) here user_id is key.
5. Resharding        		                    shard data exhaust due to uneven data distribution, update shard function move data around, consistent hashing used to solve.
6. Celebrity problem:   		                imagine Katy Perry, Justin Bieber all end-up on same shard. To solve allocate a dedicate shard for each celebrity.
7. Join and de-normalisation: 	            difficult to perform join operations across database shards.


CACHE
1. Read-through cache                                Read data from cache if not present in cache, and store in it.
2. Decide when to use cache                          Read frequent and write infrequent.
3. Expiration Policy.                                Choose wisely not too short and not too long.
4. Consistency.                                      Make synchronise data updation in data store and cache.
5. Mitigating failures(single point of failure).     Have multiple cache servers across different data centres.
6. Cache Eviction.                                   When cache is full, add request can remove existing item, can use (LFU or FIFO) to remove  
7. Cache miss                                        when data is not present in cache and had to read from database 

CDNs(Content Delivery Network)
1. Cost                    caching infrequent assets, no benefit, move out from CDN
2. Cache Expiry            set appropriate time
3. CDN fallback.           Should handle CDN failure, If fails redirect to the source origin.
4. Invalidating files.     Remove file from before it expires, by apis or versioning.   

Data Center setup:
1. Traffic redirecting:         GeoDNS is used to redirect traffic to nearest data centre.
2. Data Synchronisation:        take multiple copies in different data centres.
3. Test and Deployment:         with mufti-data centre, test application in very region.

Automated deployment process:
1. Automation:                          automate deploy code, infrastructure, configurations across env.
2. CI/CD:                               continuous testing and automated release of software.
3. Version Control Integration:         code changes are automatically deployed when merged or committed.
4. Rollback Capabilities:               quickly rollback to previous version in case of issue.
5. Multi-Env support:                   development, testing, and production, ensuring consistent deployment processes.
6. Automated tools:                     Jenkins, Ansible, Kubernetes, Terraform, Docker
7. Go critic

Message Queue:
1. Decouple components:                 component scale independently,     
2. Asynchronous Communication:          buffer and distributes asynchronous requests.
3. Producer:                            produces event/message in topic
4. Consumer:                            consumes event/message by subscribing the topic.  

Logging, metrics, automation
1. Logging:         monitoring error logs, identify errors and problems in the system.
2. Metrics:         understand health status, host metrics(cpu, ram…), Aggregate level metric (entire database, cache…), key business metrics( daily active user….)
3. Automation:      CI code check-in is verified through automation. Automate builds, test, deploy process 

System for Million of Users and beyond
1. Keep web tier stateless
2. Build redundancy at each tier.
3. Cache data as much as you can.
4. Support multiple data centres.
5. Host static content in CDNs.
6. Scale your data tier by sharing.
7. Split tiers into individual services.
8. Monitor your system ans use automated tools.

Estimate twitter QPS(query per second) and storage requirements Tips,
1. Rounding and Approximation.
2. Write down your assumptions
3. Label unit like 5MB or 5GB
4. Commonly asked back-of-the-envelope estimations:  QPS, peak QPS, storage, cache etc.


Rate Limiter 
1. Request block:                                                          2 post per sec, create 10 accounts per day per IP, claim reward no more than 2 time a week from same device.
2. Prevent resource starvation:                                            prevents DOS attacks, by blocking the excess calls. Twitter limits 300 per 3 hour.
3. Reduce Cost:                                                            important when use third party APIs,
4. Prevent server overloading
5. Client-side rare limiter or server-side rare limiter.
6. High fault tolerance:                                                    if any problems, not affect the entire system. 
7. Rate limiter middleware:                                                 place in the middle of client and the API server. Use cache(Redis) to store bucket info
8. Status code 429:                                                         too many request
9. Types of bucket we need:                                                 per API 1 bucket, per IP bucket or global bucket.
10. Token bucket:                                                           token are filled per minute like 4 tokens per min. Per request takes 1 token to execute. problem  with burst of request.
11. Leaking bucket:                                                         request are stored in queue and processed on regular intervals. Problem with burst of old request
12. Fixed window counter:                                                   divides timeline into fixed time window and assign a counter. Problem of burst of request at the edges.
13. Sliding window log algorithm:                                           maintains a timestamp of request.
14. Sliding window counter algorithm:                                       combination of fixed window and sliding window log.
15. Race condition:                                                         highly concurrent environment. Mutex or locks should be used.
16. Synchronisation issue:                                                  When using multiple rate limiter synchronisation is required. Solution use sticky session. Not advisable because of scalability issue, better solution use centralised data store like redis.

Consisting Hashing
1. Horizontal Scaling
2. Rehashing problem:                    hash(key) % N, N is number of servers, well when size of pool is fixed and data distribution is even, fails when pool updates
3. Cache Miss when pool update:          consisting Hashing solves the problem 
4. Definition:                           Consistent hashing is a special kind of hashing such that when a hash table is re-sized and consistent hashing is used, only k/n keys need to be remapped on average, k = keys, & n= slots
5. Hashing functions:                    SHA-1
6. Hash servers:                         can map based on server’s IP address 
7. Hash Keys:                            different from what used in reshaping problem.
8. Server lookup:                        go clockwise from key position on the ring until a server is found.
9. Add  or remove server:                small fraction of keys need to be redistributed.
10. Issues with it:                      partitions are not uniform, can be very small or fairly large therefore non uniform distribution of keys.
11. Virtual Nodes:                       Kind of solves the issue, virtually represent a server in the ring, by again hash the server. In real world system, number of virtual node is much larger.
12. Standard deviation:                  As no. of virtual node increases standard deviation of keys distribution to servers become small.

Key-Value Store (NOSQL database)
1. Single-Server Key value store:                         store key-value in hash tables, in memory, fitting everything in memory impossible, optimisation, data compression & frequent data. No made for big data store
2. CAP theorem:                                           impossible for a distributed system to simultaneously provide more than two of these guarantees.
    1. Consistency:                                       consistency means all clients see the same data 
    2. Availability:                                      availability means any client which requests data gets a response even if some of the nodes are down. 
    3. Partition Tolerance:                               a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. 
3. Distributed key-value store:                           also known distributed hash tables,
    1. CP (Consistency and partition tolerance) system:   if any one node unavailable, block all writes ops, all system down, Banks 
    2. AP (Availability and partition tolerance) system:  Keep accept read & write while a node down, data synced later when Network partition resolved.
4. Data Partition:                                        distribute data into multiple server, two challenge, distribute data evenly and minimise data movement when node add or remove, consisting hashing can be used here.
5. Data replication:  
    1. High availability and reliability:                 replicated data asynchronously over N servers
    2. Issue when Virtual Nodes:                          N nodes in the ring may owned by fewer than N servers, that’s why avoid storing data in same data centres, store replica in distinct data centre.
6. Consistency:                                           data replicated multiple nodes, need to sync, quorum can guarantee consistency for both read and write.
    1. Quorum:
        1. There is a coordinator that manages, there is N replicas
        2. Write quorum of size W, write ops considered success if ack from W replicas.
        3. Read quorum of size R, read ops considered success if ack from at least R replicas.
        4. Configuration N,W,R typical tradeoff between latency and consistency.
        5. R=1 and W = N, the system is optimised for a fast read. 
        6. W=1 and R = N, the system is optimised for fast write. 
        7. W + R > N, strong consistency is guaranteed, at least one overlapping node that maintains latest data. 
        8. W + R <= N, strong consistency is not guaranteed. 
    2. Strong Consistency:                                forcing a replica not to accept new reads/writes until every replica has agreed on current write, not suitable for highly available system.
    3. Weak Consistency:                                  subsequent read operations may not see the most updated value. 
    4. Eventual Consistency:                              this is a specific form of weak consistency. Given enough time, all updates are propagated, and all replicas are consistent.  Dynamo and Cassandra adopt eventual consistency, which is our recommended consistency model for our key-value store. 
    5. Inconsistency Resolution:                          versioning
        1. Replication gives high availability but causes inconsistencies among replicas. Versioning and vector locks are used to solve inconsistency problems 
        2. Vector Locks:    A vector clock is a [server, version] pair associated with a data item 
7. Handling Failures:
    1. Gossip Protocol:                                   Each node has member id and heartbeat counter, periodically increment the counter, and sends this to random nodes and that updates the membership list, if heartbeat not inc within predefined period, considered offline.
8. Handling temporary failures:                           use sloppy quorum.
9. System Architecture:
    1. Client comm. with key-value store through APIs, get(key), put(key, value)
    2. A coordinator is a node acts as a proxy between client and key-value store.
    3. Nodes are distributed on the ring using consistent hashing.
    4. The system is decentralised so adding removing nodes become automatic.
    5. Data is replicated multiple nodes, no single point of failure.
10. Write path:                                           (based on the Architecture of Cassandra)
    1. Write request is persisted on a commit log file.
    2. Data is saved in memory cache, when cache is full or reached to a threshold, data is fused into SS Tables( sorted-string table), sorted list of key-value pairs.
11. Read Path
    1. Checks first data is in memory cache, return directly 
    2. If not, check in bloom filters to figure out which SSTables and then return its value.
12. Summary
	1. Ability to store big data:                             use consistent hashing to spread the load across multiple servers.
	2. High availability reads:                               Data replication, multi data-centre setup.
	3. High available writes:                                 versioning and conflict resolution with vector clocks
	4. Dataset partition:                                     consistent Hashing 
	5. Incremental Scalability:                               consistent Hashing.
	6. Heterogeneity:                                         consistent Hashing.
	7. Tunable Consistency:                                   quorum consensus.
	8. Handling temporary failures:                           sloppy quorum and hinted handoff
	9. Handling permanent failures:                           Merkle tree
	10. Handling data centre outage:                          cross data-centre replication.


Unique ID generator:
1. Multi-master replication		  increase it by k, k = no. of databases, hard to scale also not possible if server cnt updates. IDs don;t go up to time.
2. Universally unique identifier (UUID)   128bit, don't go up based on time, alpha numeric number, large space to store.
3. Ticket server            		  centralized auto_increment feature in a single database server. easy to implement and scale but single point of failure.
4. Twitter snowflake 			  sign bit(1) + timestamp (41)epoch time(69 years) + datacenterID (5) + MachineID (5) + Sequence number (12). Best for distributed networks.

URL Shortner
1. URL redirecting 		when you enter a tinyurl onto the browser. Once the server receives a tinyurl request, it changes the short URL to the long URL with 301 redirect.
2. API endpoints:  		URL shortening:= POST api/v1/data/shorten, URL redirecting:= GET api/v1/shortUrl.
3. 301 Vs 302 redirect 		permanently redirected therefore browser caches and reduces load as only 1st request go | temporaraly as request first send to shortening server then redirected better when track click rate.
4. Hash Maps			stored in url shorting server: <short Url, Long url>
5. URL Shortening 		long url -----> hash  = https://tinyUrl/{hashValue}.


