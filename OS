* What is the main purpose of an operating system? Discuss different types?
    The main purpose of operating system is to act as an intermediary between computer hardware and the user, managing hardware resources, providing user interface and enabling execution of 
    applications. the OS ensures fair use of resources(CPU,RAM,Storage etc), manages file system, handles I/O, ensures security and stablity.
    * Types of OS 
        1) Batch OS:
            process batches of jobs without user interaction, ideal for repeititive task. eg Early IBM system
        2) Time Sharing OS:
            Allows multiple users to interact with the computer simultaneously. Provides each user with a small time slice for CPU usage. eg UNIX
        3) Distributed OS:
            manages a group of distinct computer and make them appear as a single system. Enables resource sharing and improved performance accross a network. eg Amoeba, LOCUS.
        4) Network OS:
            Provides networking capabilities to manage data, users, and security over a network. eg Windows Server,
        5) Real-Time OS:
            Process data in real-time, with strict time constraints, Used for applications where time-critical response is essential. eg VxWorks
        6) Mobile OS:
            Designed specially for mobile devices. Optimizes performance, battery life, and user experience for mobile platforms. eg Android, iOS.

* What is a socket, kernel and monolithic kernel ? 
    * Socket:
        A socket is an endpoint for sending or recieving message over a network.
        Sockets allow application to communicate with each other using standard protocol like TCP or UDP.
        Messaging Apps use sockets, particularly TCP or WebSockets, to facilitate real-time communication, ensuring that messages are delivered quickly and reliably between users.
    * Kernel:
        It is the core component of an operating system that manages system resource and communication between hardware and software.
        It handles task such as process management, memory management, device management, and system calls.
    * Monolithic kernel:
        A monolithic kernel is a type of kernel architecture where the entire operating system runs in a single kernel space, including device drivers, file system management, and system calls.
        Provides good performance but lots of lines of code. e.g, Unix, Linux.

* Difference between process and program and thread? Different types of process ? 
    * Program:
        A program is a set if instruction written in a programming language that is stored in a disk.
        eg. a C file example.c or a executable file example.exe
    * Process:
        process is an instance of a program. When execute a program it becomes a process which performs all the tasks meantioned in the program.
        eg. When you open a browser (e.g., chrome.exe), an instance of the program is created as a process.
    * Thread:
        A thread is a smallest unit of execution in a process.It is also called a lightweight process. The idea is to divide the process into multiple process to achieve parallelism.
        eg. Word processor uses multiple threads: one thread to format the text, another thread to 

* Define virtual memory, thrashing, threads.
    * virtual memory:
        It is a memory management technique used by the operating system to provide an "isllusion" of a large, contiguous block of memory to programs, even the physical memory(RAM) is smaller.
        It used combination of RAM and disk storage(HDD or SDD) to extend the apparent avialable memory.
        When a program needs more memory than is physically available, parts of data that are not immediately needed is swapped out to disk. and the required data is loaded to the RAM.
        Provides isolation between processes, enhancing security and stability.
    * thrashing:
        It is a situation when system is spending a major portion of its time in serving page faults. But the actual processing is done very low.
        High degree of multiprogramming (if number of process keeps on increasing in the memory), if a process is allocated too few frames, then there will too frequent page faults. causing thrashing
        if a computer has too many applications open with high memory demands, it may start thrashing as the operating system continuously swaps data between RAM and disk.

* What is RAID ? Different types ?
    Redundant array of independent disk is a technique which makes use of combination of multiple disk instead of using a single disk to increase performance, data redunduncy or both.
    It distributes data across the disks in various ways, depending on the RAID level, providing different balances of speed, storage capacity, and fault tolerance.
    Types of RAID:
        * RAID 0 (Striping):
            Data is split across multiple disks. high performance because of parallel read and write but no redunduncy.
            If one disk fails all data is lost.
        * RAID 1 (Mirroring):
            Data is duolicated across multiple disks. High data reliablity; redunduncy.
            Uses twiced the storage capacity.
        * RAID 10 (striping + mirroring)
            Combines RAID 1 and RAID 0 (mirroring and striping). High performance and fault tolerance.
        * RAID 3 (striping with single parity):
            Data is striped with parity information stored in a single disk, so that if one striped disk fail can retreive its data from parity.
            Slow writes as parity also need to be update in case of update so parity disk acts a bottleneck here as lot of throughput to handle.
        * RAID 5 (striping with distributed parity)
            Data is striped with parity information distributed among disks. Good balance of performance, storage, and fault tolerance.
            Can recover from only one disk failure.
        * RAID 6 (striping with double parity)
            Similar to RAID 5 but with double parity. Can survive up to two disk failures. 
            Slower write performance.
        * RAID 50 and RAID 60:
            Combine multiple RAID 5 or RAID 6 arrays with RAID 0.

* What is a deadlock? Different conditions to achieve a deadlock. 
    It is a situation in multi-process system where two or more process not able to procced because one is waiting for a resource that the other one holds.
    Conditions of deadlock:
        A deadlock occurs when the following four conditions are met simultaneously:
        * Mutual exclusion:
            Only one process can use resource at a time. If another process request for the resource it should wait.
        * Hold and wait:
            A process must be holding atleast one resource and simultaneously waiting for additional resources that are currently held by the other processes.
        * No preemption:
            Resources cannot be forcebly taken from the process holding them; they must be released voluntarily by the holding process after completion of the task.
        * Circular wait:
            A circular chain of processes exists, where each process holds at least one resource needed by the next process in the chain. 
            For example, Process A is waiting for a resource held by Process B, Process B is waiting for a resource held by Process C, and Process C is waiting for a resource held by Process A.
    A deadlock occurs when these four conditions coexist in a system. Managing deadlocks typically involves prevention, detection, or recovery strategies to avoid deadlock.

* What is fragmentation? Types of fragmentation. 
    Fragmentation is the inefficient use of memory or storage, where free space is divided into small, unusable pieces, affecting system performance.
    Types of fragmentation:
        Internal fragmentation:
            Occurs when fixed-sized memory blocks are allocated, and the memory requested by the process is smaller than the allocated block size. 
            The unused space within the allocated block is wasted.
        External fragmentation:
            Happens when free memory is scattered in small, non-contiguous blocks across the system.
            Even though there may be enough total free memory to satisfy a request, the memory is not contiguous, preventing allocation

* What is spooling ?
    Simulataneous peripheral operations online is a process where data is stored in a buffer (usually in a disk) to be processed or send to the peripheral devices like printer or any I/O devices.
    Purpose of Spooling:
        Improves performance: Allows CPU to continue working while peripheral device process data stored in disk.
        Manages Device Access: Coordinates access to slower devices shared by multiple users or processes.
        Prevents Blocking: Enables multiple tasks to be executed simultaneously without waiting for one device to finish.
    example: Print Spooling: When multiple print jobs are sent to a printer, they are stored in a spool (a buffer area on the disk). 
    The printer processes each job sequentially from the spool while the CPU continues other tasks without waiting for the printer to complete.

* What is semaphore and mutex ? differentiate also Define Binary semaphore.
    Semaphore:
        A synchronization tool that uses a counter to access the shared content.
        Types:
            Counting Semaphore: Allows multiple threads to access resources up to a set limit.
            Binary Semaphore: Functions like a mutex, allowing only one thread to access a resource at a time.
    Mutex:
        A locking mechanism that ensures only one thread can access a resource at a time.
        The thread that locks the mutex must be the one to unlock it.

* Define Belady’s Anomaly ?
    Belady’s Anomaly is a situation where increasing the number of page frames in memory can actually lead to an increase in the number of page faults
    when using the FIFO (First-In-First-Out) page replacement algorithm.
    * Solution to fix Belady’s Anomaly:
        Implementing alternative page replacement algo helps eliminate Belady’s Anomaly.. Use of stack based algorithms, such as Optimal Page Replacement Algorithm 
        and Least Recently Used (LRU) algorithm, can eliminate the issue of increased page faults as these algorithms assign priority to pages

* what are Starving and Aging in OS ?
    Starvation:
        A situation where a process is contineously denied nessasary resources because other processes are always given priority.
        Can occur in scheduling systems where high-priority tasks preempt lower-priority ones.
    Aging:
        A technique used to prevent starvation by gradually increasing the priority of processes that have been waiting for a long time.
        Ensure long waiting process get a chance to execute.
    starvation is the indefinite delay of processes, while aging is a method to prevent it by boosting the priority of waiting processes over time.

* Why does thrashing occurs ?
    High degree of multiprogramming(if number of proccess keeps on increasing in memory), lack of frames(if a process is allocated too few frames) then 
    there will be too many and too frequent page faults.) causing thrasing.

* What is paging and why we need it ?
    paging is a memory management scheme that eleminates the need for contiguous allocation of physical memory. Operating system divides the memory into 
    blocks called frames. Each process is also divided into pages of the same size, allowing the OS to load them into any available page frames in memory.
    Paging allows processes to use non-contiguous memory, reducing fragmentation and maximizing memory utilization.
    Paging is essential to implement virtual memory, which allows a process to use more memory than is physically available, by swapping pages between 
    RAM ans disk.

* What is Demand Paging and segmentation ?
    * Demand Paging 
        It is a memory management technique where pages of a process are loaded only when needed(on demand), rather than loading the entire process into memory once
        Reduces the amount of memory used and minimizes loading time by only loading pages that are actually required by the process.
    * Segemntation 
        It is a memory management technique where memory is divided into variable size parts known as segments and allocated to a process.
        The details about each segment are stored in a table called a segment table. Segment table is stored in one (or many) of the segments.
        Segment table contains mainly two information about segment:
        Base: It is the base address of the segment
        Limit: It is the length of the segment.

* what is real-time operating system and what are its types ?
    A real-time operating system (RTOS) is a special-purpose operating system used in computers that has strict time constraints for any job to be performed 
    and is intended to serve real time applications that possess data as it comes in , typically without buffer delays.
    Types of RTOS:
        Hard RTOS:
            Guarantees that critical tasks are completed within a strict time limit.
             Used in mission-critical applications like medical devices, aircraft control systems
        Soft RTOS:
            Prioritizes critical tasks but allows for some flexibility in meeting deadlines.
            Missing a deadline may degrade performance but does not cause system failure.
            Used in multimedia applications, telecommunications, and other scenarios where delays are undesirable but not critical.
        Firm RTOS:
            A mix between hard and soft RTOS. Missing a deadline is undesirable but does not result in a complete system failure.        
            Used in systems like online transaction processing or some industrial systems where occasional deadline misses are tolerable.

* what is Static and dynamic binding ?
    Static binding happens when the code is compiled, while dynamic bind happens when the code is executed at run time.

* Explain FCFS First-Come, First-Served) Scheduling ?
    It is a simple CPU scheduling algorithm where the process that arrives first gets executed first.
    It is a non-preemptive scheduling algorithm, meaning once a process starts its execution, it cannot be stopped until it completes.
    Longer processes can lead to the "Convoy Effect," where shorter processes have to wait for longer ones to complete.
    Lack of preemption can lead to long wait times and reduced responsiveness.
    Process	Arrival Time	Burst Time
    P1	        0	        5
    P2	        1	        3
    P3	        2	        2

    Execution Order: P1 → P2 → P3
    Waiting Time:
    P1: 0
    P2: 4 (P1's burst time)
    P3: 7 (P1 + P2's burst times)

* Explain SJF (Shortest Job first) 
    SJF (Shortest Job First) is a CPU scheduling algorithm that selects the process with the shortest burst time
    (i.e., the least time required to execute) from the ready queue for execution. It can be either preemptive or non-preemptive:
        * Non-preemptive SJF: Once a process starts executing, it runs until completion.
        * Preemptive SJF (Shortest Remaining Time First, SRTF): If a new process arrives with a shorter burst time than the remaining time of the current process, the CPU switches to the new process.
    Efficient use of CPU time as shorter tasks finish quickly, allowing more processes to be executed in less time.
    Starvation: Longer processes may suffer from indefinite waiting if shorter processes keep arriving. 

    Process	Arrival Time	Burst Time
    P1	              0	        7
    P2	              2	        4
    P3	              4	        1
    P4	              5	        4

    Execution Order (Non-preemptive SJF): P1 → P3 → P2 → P4

    Execution Order (Preemptive SJF / SRTF):
    P1 starts first (since it arrives at 0).
    At time 2, P2 arrives but doesn't preempt P1.
    At time 4, P3 arrives and preempts P1 (since P3 has the shortest burst time).
    After P3 completes, the next shortest job, P2, is executed.
    Finally, P4 is executed.

* Explain LRTF (Longest remaining time first)
    It is preemptive CPU scheduling algorithem where the process with the longest remaining burst time is selected for execution.     
    If a new process arrives with a burst time longer than the remaining time of the currently running process, the current process is preempted, and the new process is scheduled.
    Unlike SJF, longer processes are prioritized, preventing them from being starved by shorter ones.
    Can lead to higher average waiting times since shorter processes may have to wait for longer processes to complete. 
    Process	Arrival Time	Burst Time
    P1	            0	        4
    P2	            1	        6
    P3	            2	        5
    At time 0, P1 starts execution (no other process has arrived).
    At time 1, P2 arrives with a longer burst time (6) than P1's remaining time (3), so P2 preempts P1.
    At time 2, P3 arrives with a burst time (5) that is less than P2's remaining time (6), so P2 continues executing.
    P2 continues until completion since it has the longest remaining time.
    Then P3 is executed because it has more remaining time than P1.
    Finally, P1 is executed.

* Explain Priority Scheduling 
    Priority Scheduling is a CPU scheduling algorithm where each process is assigned a priority. The CPU is allocated to the process with the highest priority
    Preemptive Priority Scheduling: If a new process with a higher priority arrives, it can interrupt the current process.
    Non-preemptive Priority Scheduling: The CPU finishes the current process, even if a higher-priority one arrives.
    Allows important or time-sensitive processes to run first.
    Can cause starvation: lower-priority processes might never get executed.

* Explain Round Robin Scheduling.
    It is CPU scheduling algorithm where each process is assigned a fixed time slice or quantum. The CPU cycles through the ready processes in a circular order,
    given each process to execute for a fixed period. All processes get an equal share of CPU time.
    Frequent switching between processes can reduce efficiency.
    Too small a quantum leads to excessive context switching; too large reduces responsiveness.

* Explain Producer Consumer Problem.
    The Producer-Consumer problem is a classic problem that is used for multi-process synchronisation i.e. synchronisation between more than one processes.    
    The job of the Producer is to generate the data, put it into the buffer, and again start generating data. While the job of the Consumer is to consume the data from the buffer.
    What's the problem here?
    The following are the problems that might occur in the Producer-Consumer:
    The producer should produce data only when the buffer is not full. If the buffer is full, then the producer shouldn't be allowed to put any data into the buffer.
    The consumer should consume data only when the buffer is not empty. If the buffer is empty, then the consumer shouldn't be allowed to take any data from the buffer.
    The producer and consumer should not access the buffer at the same time.
    Solution:
    Use synchronization mechanisms like semaphores and mutexes:
    
    Mutex ensures that only one process (producer or consumer) accesses the buffer at a time.
    Semaphores (empty and full) track the number of empty and filled slots in the buffer.

* What is Banker's Algorithm ?
    The Banker’s Algorithm is a deadlock avoidance algorithm used in operating systems to manage resource allocation among multiple processes.
    Before granting a resource request, the system checks if allocating the resources will leave the system in a "safe state."
    A safe state means that there is a sequence of processes such that each can obtain its maximum resources and complete its execution.
    If the request leads to an unsafe state (where deadlock could occur), the request is denied.
    The banker's algorithm is named because it checks whether a person should be sanctioned a loan amount or not to help the bank system safely simulate allocation resources.

* Explain Cache.
    Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU.
    It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.

* Diff between direct mapping and associative mapping ?
    Direct Mapping and Associative Mapping are two techniques used in caching to determine where data is stored in the cache memory.
    Direct Mapping:
        In direct mapping, each block of main memory maps to exactly one location in the cache.
        The cache is divided into multiple blocks, and each block of memory is mapped to a specific cache block based on a modulo operation (Memory Block Number % Number of Cache Blocks).
        Multiple memory blocks map to the same cache block, causing frequent replacements (higher collision rate).
        Potentially higher miss rates due to limited placement options.
     Associative Mapping:
        In associative mapping, any block of main memory can be loaded into any block of the cache.
        The cache does not have a fixed location for a particular memory block. Instead, it uses a search mechanism (often a tag comparison) to determine if a block is present in any cache line.
        More complex hardware required for searching (tag comparisons) since each lookup involves searching the entire cache

* Diff between multitasking and multiprocessing ?
    Multitasking:
        Multitasking is the ability of an operating system to execute multiple tasks or processes seemingly simultaneously on a single CPU.
        The CPU switches between tasks rapidly (context switching) to give the illusion that multiple tasks are running concurrently. Only one task is processed at any given moment.
        Improves responsiveness and user experience by running multiple applications at once.
        Running a web browser, music player, and text editor simultaneously on a single-core CPU.
    Multiprocessing:
        Multiprocessing is the ability of a system to use two or more CPUs (processors) to execute multiple processes simultaneously.
        Each processor runs its own process independently; multiple processes can be executed in parallel, truly at the same time.
        Increases computational power, throughput, and reliability by running processes in parallel.
        High-performance servers, parallel computing systems, and multi-core processors in modern computers.

* What are system calls ?
    System calls are like a bridge between a program and the operating system.
    When a program needs to perform a task that requires the OS, like reading a file or sending data, it uses a system call to ask the OS for help.
    Think of it like a program "making a request" to the OS to do something specific.

* What is a kernel ?
    Kernel is the first program that is loaded in memory when OS is loading as well as it remains in memory till OS is running.
    Kernel is the core part of the OS which is responsible for managing resources, allowing multiple processes to use the resources and provide services to various processes.
    Kernel modules can be loaded and unloaded in run-time i.e. in running OS.

*  If a process fails, most operating system write the error information to a ?
    If a process fails, most operating systems write the error information to a log file. Log file is examined by the debugger, to find out what is the actual cause of that particular problem.
    Log file is useful for system programmers for correcting errors.



